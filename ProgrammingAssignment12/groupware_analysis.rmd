---
title: 'Groupware Accelerometer Data Analysis'
author: "Tolu Omotunde"
date: "29/Sep/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
mode: selfcontained
hitheme: tomorrow
highlighter: highlight.js
widgets: mathjax
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## Abstract  
This report uses a dataset which generated by accelerometers in different body parts of participants who took part in an exercise.  
  
## Loads the libraries used in this report  
```{r, message=FALSE, cache=TRUE}
library(caret)
set.seed(51971)
```

## Loads the dataset into memory  
```{r, cache=TRUE}
train_data <- read.csv("data/pml-training.csv")
test_data <- read.csv('data/pml-testing.csv')
```  
  
## Perform some exploration on answering structure  
```{r, cache=TRUE}
'problem_id' %in% names(test_data)
'problem_id' %in% names(train_data)
'classe' %in% names(test_data)
'classe' %in% names(train_data)
unique(train_data$classe)
```
## Data cleaning  
After performing some other exploratory analysis on the dataset, I decided to use only the sensor measurements for prediction.  
* Removes useless variables  
```{r, cache=TRUE}
variables_to_be_removed <- c(
    'X',
    'user_name',
    'raw_timestamp_part_1',
    'raw_timestamp_part_2',
    'cvtd_timestamp',
    'new_window',
    'num_window'
)
remove_columns_by_name <- function (dataset, columns) {
    for (variable in columns) {
        dataset[variable] <- NULL
    }
    dataset
}
dim(train_data)
dim(test_data)
train_data <- remove_columns_by_name(train_data, variables_to_be_removed)
test_data <- remove_columns_by_name(test_data, variables_to_be_removed)
dim(train_data)
dim(test_data)
```
  
* Gets the problem_id(s) from the test dataset and classe from the train dataset  
```{r, cache=TRUE}
ids <- as.data.frame(test_data$problem_id)
labels <- as.data.frame(as.factor(train_data$classe))
train_data <- subset(train_data, select = -c(classe))
test_data <- subset(test_data, select = -c(problem_id))
```
  
* Converts all values to numeric* Convert values in the dataset into numeric ones  
```{r, message=FALSE, cache=TRUE}
convert_to_numeric <- function(x) {
    as.numeric(as.character(x))
}
numeric_factor <- function(dataset) {
    data.frame(sapply(dataset, function (col) {
        convert_to_numeric(col)
    }))
}
train_data <- numeric_factor(train_data)
test_data <- numeric_factor(test_data)
```
  
* Center and scale  
```{r, cache=TRUE}
preprocessed_train <- preProcess(train_data, method=c('center', 'scale'))
train_data <- predict(preprocessed_train, train_data)
test_data <- predict(preprocessed_train, test_data)
```
  
* Deals with missing values, remove columns with zero variance  
```{r, cache=TRUE}
variables_to_be_removed <- c(
    'kurtosis_yaw_belt',
    'skewness_yaw_belt',
    'kurtosis_yaw_dumbbell',
    'skewness_yaw_dumbbell',
    'kurtosis_yaw_forearm',
    'skewness_yaw_forearmThese',
    'amplitude_yaw_belt',
    'amplitude_yaw_dumbbell',
    'amplitude_yaw_forearm'
)
train_data <- remove_columns_by_name(train_data, variables_to_be_removed)
test_data <- remove_columns_by_name(test_data, variables_to_be_removed)
median_missing <- function (dataset) {
    dataset <- dataset[, colSums(is.na(dataset)) == 0]
    dataset
}
train_data <- median_missing(train_data)
test_data <- median_missing(test_data)
dim(train_data)
dim(test_data)
```
  
* Dimension reduction using PCA  
```{r, cache=TRUE}
reduced_data <- preProcess(train_data, method='pca', thresh=0.90)
train_data <- predict(reduced_data, train_data)
test_data <- predict(reduced_data, test_data)
dim(train_data)
dim(test_data)
```
  
* Re-attaches labels to the train data  
```{r, cache=TRUE}
names(labels) <- 'labels'
train_data <- cbind(labels, train_data)
dim(train_data)
```
  
* Cross validation  
For this step, I split the training dataset into two parts with ratio of 7:3.  
```{r, cache=TRUE}
splited <- createDataPartition(y=train_data$labels, p=0.7, list=FALSE)
cross_train <- train_data[splited, ]
cross_test <- train_data[-splited, ]
dim(cross_test)
dim(cross_train)
```
  
## Model validation
#### I am going to run 3 rpart, rf and lda.The goal is to pick the best performing model
  
```{r, echo=FALSE, cache=TRUE}
available_algorithms = c('rpart', 'rf', 'lda')
```
 
 ### Random partitioning trees
```{r, echo=FALSE, cache=TRUE}
my_algorithm = available_algorithms[1]
model_rpart <- train(labels ~ ., data = cross_train, method = my_algorithm)
predict_rpart<- predict(model_rpart, cross_test)
confusionMatrix(as.factor(cross_test$labels), predict_rpart)
```
 
 ### Linear discriminatory analysis
```{r, echo=FALSE, cache=TRUE}
my_algorithm = available_algorithms[3]
model_lda <- train(labels ~ ., data = cross_train, method = my_algorithm)
predict_lda<- predict(model_lda, cross_test)
confusionMatrix(as.factor(cross_test$labels), predict_lda)
```
 
 ### Random Forest
```{r, echo=FALSE, cache=TRUE}
my_algorithm = available_algorithms[2]
model_rf <- train(labels ~ ., data = cross_train, method = my_algorithm)
predict_rf<- predict(model_rf, cross_test)
confusionMatrix(as.factor(cross_test$labels), predict_rf)
```




```{r, cache=TRUE}
predict_rf
```
  
* Calculate the accuracy  
```{r, cache=TRUE}
accuracy <- sum((predict_rf == cross_test$label)) / length(cross_test$label)
print(paste('Accuracy is:', accuracy))
```
  
With the RF, the result is 97%, that's high!  